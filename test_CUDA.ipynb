{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4809c047",
   "metadata": {},
   "source": [
    "# Micro-benchmark: matrix multiply (GFLOPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc7524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "N=2048 | avg_time=   0.88 ms | throughput≈ 19.49 TFLOPS\n",
      "N=3072 | avg_time=   2.71 ms | throughput≈ 21.37 TFLOPS\n",
      "N=4096 | avg_time=   5.81 ms | throughput≈ 23.66 TFLOPS\n",
      "N=6144 | avg_time=  19.45 ms | throughput≈ 23.85 TFLOPS\n"
     ]
    }
   ],
   "source": [
    "# file: torch_gflops.py  | jalankan: python torch_gflops.py\n",
    "import torch, time, math, os, platform\n",
    "assert torch.cuda.is_available(), \"GPU CUDA tidak terdeteksi oleh PyTorch.\"\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def bench_mm(n=4096, iters=30, warmup=10):\n",
    "    a = torch.randn((n,n), device=\"cuda\", dtype=torch.float16)  # FP16 lebih cepat (jika mendukung)\n",
    "    b = torch.randn((n,n), device=\"cuda\", dtype=torch.float16)\n",
    "    # Casting ke FP32 output untuk stabilitas (opsional)\n",
    "    # a = a.float(); b = b.float()\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        c = a @ b\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.time()\n",
    "    for _ in range(iters):\n",
    "        c = a @ b\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "\n",
    "    avg_s = (t1 - t0) / iters\n",
    "    # FLOPs untuk gemm: 2*N^3\n",
    "    tflops = (2 * (n**3)) / avg_s / 1e12\n",
    "    return avg_s, tflops\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "    for size in [2048, 3072, 4096, 6144]:\n",
    "        avg_s, tflops = bench_mm(n=size)\n",
    "        print(f\"N={size:4d} | avg_time={avg_s*1000:7.2f} ms | throughput≈{tflops:6.2f} TFLOPS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58338e81",
   "metadata": {},
   "source": [
    "# Benchmark inferensi model (images/sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714a1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Batch=128 | avg_time_per_batch=320.28 ms | throughput≈399.7 images/sec\n"
     ]
    }
   ],
   "source": [
    "# file: torch_resnet50_bench.py | jalankan: python torch_resnet50_bench.py\n",
    "import torch, time\n",
    "import torchvision.models as models\n",
    "\n",
    "assert torch.cuda.is_available(), \"GPU CUDA tidak terdeteksi.\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = models.resnet50(weights=None).cuda().eval()   # tanpa weights untuk kecepatan init\n",
    "batch_size = 128\n",
    "warmup, iters = 10, 30\n",
    "\n",
    "x = torch.randn(batch_size, 3, 224, 224, device=\"cuda\")\n",
    "\n",
    "# Warmup\n",
    "with torch.inference_mode():\n",
    "    for _ in range(warmup):\n",
    "        y = model(x)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Timing\n",
    "t0 = time.time()\n",
    "with torch.inference_mode():\n",
    "    for _ in range(iters):\n",
    "        y = model(x)\n",
    "torch.cuda.synchronize()\n",
    "t1 = time.time()\n",
    "\n",
    "avg_s = (t1 - t0) / iters\n",
    "imgs_per_sec = batch_size / avg_s\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "print(f\"Batch={batch_size} | avg_time_per_batch={avg_s*1000:.2f} ms | throughput≈{imgs_per_sec:.1f} images/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbdb8e",
   "metadata": {},
   "source": [
    "# Tes memory bandwidth (opsional, cepat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0394f195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy ~2.00 GB | time=0.7256s | bandwidth≈2.76 GB/s\n"
     ]
    }
   ],
   "source": [
    "# file: torch_bandwidth.py | jalankan: python torch_bandwidth.py\n",
    "import torch, time\n",
    "x = torch.empty(2_000_000_000//4, dtype=torch.float32, device=\"cuda\")  # ~2GB/4=0.5G elemen? (sesuaikan VRAM!)\n",
    "torch.cuda.synchronize()\n",
    "t0 = time.time(); y = x.clone(); torch.cuda.synchronize(); t1 = time.time()\n",
    "gb = x.numel()*x.element_size()/1e9\n",
    "bw = gb/(t1-t0)\n",
    "print(f\"Copy ~{gb:.2f} GB | time={t1-t0:.4f}s | bandwidth≈{bw:.2f} GB/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a587d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d219ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8493b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat tensor acak besar\n",
    "a_cpu = torch.randn(size, size)\n",
    "b_cpu = torch.randn(size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64dce411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CPU Timing ---\n",
    "start_cpu = time.time()\n",
    "result_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "end_cpu = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2954e803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 20.5007 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"CPU time: {end_cpu - start_cpu:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Pindahkan tensor ke GPU\n",
    "    a_gpu = a_cpu.to('cuda')\n",
    "    b_gpu = b_cpu.to('cuda')\n",
    "\n",
    "    # Warm-up (karena kadang operasi pertama lambat karena init CUDA context)\n",
    "    _ = torch.matmul(a_gpu, b_gpu)\n",
    "\n",
    "    # --- GPU Timing ---\n",
    "    torch.cuda.synchronize()  # pastikan semua selesai sebelum mulai timing\n",
    "    start_gpu = time.time()\n",
    "    result_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    end_gpu = time.time()\n",
    "\n",
    "    print(f\"GPU time: {end_gpu - start_gpu:.4f} seconds\")\n",
    "\n",
    "    # Optional: bandingkan hasilnya\n",
    "    diff = torch.norm(result_gpu.cpu() - result_cpu)\n",
    "    print(f\"Difference (should be close to 0): {diff:.4f}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fdcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "CUDA version: 11.8\n",
      "device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"device:\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e69d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trycuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
